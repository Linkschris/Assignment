{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from scipy.stats import poisson\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import defaultdict\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "import community\n",
    "import powerlaw\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_counter = 1\n",
    "figure_counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the strategy you have used to extract the hyperlinks from the Wiki-pages, assuming that you have already collected the rapper pages with the Wikipedia API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Since we have already downloaded all rappers pages and a csv file with rappers distinguished by coasts, we go through each rapper in the csv file and find the correspondent wikipedia file. To do this we had to transform the name of the rapper to a readable format that coincide to the name of the file.\n",
    "\n",
    "For each wikipedia file we search for the names inside brackets `[[  ]]`, which corresponds to the connections inside a wikipedia page (hyperlinks).\n",
    "\n",
    "After getting the content between the brackets, we faced the problem of having inside the same bracket two names, the one with interest was the first that appeared. E.g.: ['Hip hop music|Hip hop'], we only want 'Hip hop music'. Those names are seperated by a `|` (pipe) symbol.\n",
    "\n",
    "To get rid of all hyperlinks, which are not leading to another rapper, we check if the target of the hyperlink is in the rappers-list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the regular expressions you have built and explain in details how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Answer:**  \n",
    "   \n",
    " The Regular Expression we have used is as follows: `r'\\[\\[(.*?)(?:\\||\\]\\])'`\n",
    "\n",
    "This regular expression is designed to match and capture text within double square brackets `[[ ]]`, stopping at the first occurrence of a pipe `|` or the closing double square brackets.\n",
    "\n",
    "- `\\[`: Matches a literal `[` character.\n",
    "- `\\[\\[`: Matches the string `[[`.\n",
    "- `(.*?)`: Captures any character (.) as few times as possible (*?), making a non-greedy match.\n",
    "- `(?:\\||\\]\\])`: A non-capturing group matching either a `|` character or the string `]]`.\n",
    "\n",
    "[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Network visualization and basic stats\n",
    "\n",
    "Visualize your network of rappers (from lecture 5) and calculate stats (from lecture 4 and 5). For this exercise, we assume that you have already generated the network and extracted the largest weakly connected component (the \"largest weakly connected component\" of a directed network is the subgraph consisting of the nodes that would constitute the largest connected component if the network were undirected) . The visualization and statistics should be done for the largest weakly connected component only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create the Network, extract the  largest connected component and save it as a file \n",
    "# we include this to show how we created the network, if someone wants to run notebook without having the files.\n",
    "\n",
    "def get_wikipedia_content(title):\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title\n",
    "    }\n",
    "    response = requests.get(baseurl, params=params).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    content = page[\"revisions\"][0][\"*\"]\n",
    "    return content\n",
    "\n",
    "def get_links_and_content(wikipedia_titles):\n",
    "    links_dict = {}\n",
    "    content_dict = {}\n",
    "    for title in wikipedia_titles:\n",
    "        content = get_wikipedia_content(title)\n",
    "        links = re.findall(r'\\[\\[(.*?)(?:\\||\\]\\])', content)\n",
    "        links_dict[title] = links\n",
    "        content_dict[title] = content\n",
    "    return links_dict, content_dict\n",
    "\n",
    "def build_graph(links_dict, all_rappers_set):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(all_rappers_set)\n",
    "    for rapper, links in links_dict.items():\n",
    "        valid_links = set(links) & all_rappers_set\n",
    "        G.add_edges_from((rapper, link) for link in valid_links)\n",
    "    return G\n",
    "\n",
    "EastCoast = pd.read_csv(\"EastCoastRappers.csv\")\n",
    "WestCoast = pd.read_csv(\"WestCoastRappers.csv\")\n",
    "\n",
    "toprapper_east = EastCoast.WikipediaPageName.tolist()\n",
    "toprapper_west = WestCoast.WikipediaPageName.tolist()\n",
    "all_rappers_set = set(toprapper_east + toprapper_west)\n",
    "\n",
    "east_links_dict, east_content_dict = get_links_and_content(toprapper_east)\n",
    "west_links_dict, west_content_dict = get_links_and_content(toprapper_west)\n",
    "\n",
    "all_links_dict = {**east_links_dict, **west_links_dict}\n",
    "all_content_dict = {**east_content_dict, **west_content_dict}\n",
    "\n",
    "Graph_rapper = build_graph(all_links_dict, all_rappers_set)\n",
    "\n",
    "largest_cc_rapper = max(nx.weakly_connected_components(Graph_rapper), key=len)\n",
    "Graph_rapper_lcc = Graph_rapper.subgraph(largest_cc_rapper)\n",
    "\n",
    "for node in Graph_rapper_lcc.nodes():\n",
    "    Graph_rapper_lcc.nodes[node][\"coast\"] = \"east\" if node in toprapper_east else \"west\"\n",
    "    Graph_rapper_lcc.nodes[node][\"length\"] = len(re.findall(r'\\b\\w+\\b', all_content_dict[node]))\n",
    "    Graph_rapper_lcc.nodes[node][\"content\"] = all_content_dict[node]\n",
    "\n",
    "nx.write_gexf(Graph_rapper_lcc, \"rapper_graph.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a: Stats (see lecture 4 and 5 for more hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph\n",
    "G = nx.read_gexf(\"rapper_graph.gexf\")\n",
    "\n",
    "# 1. Number of nodes\n",
    "num_nodes = G.number_of_nodes()\n",
    "\n",
    "# 2. Number of links\n",
    "num_links = G.number_of_edges()\n",
    "\n",
    "# 3. Top connected rapper\n",
    "in_degrees = dict(G.in_degree())\n",
    "out_degrees = dict(G.out_degree())\n",
    "top_rapper_in = max(in_degrees, key=in_degrees.get)\n",
    "top_rapper_out = max(out_degrees, key=out_degrees.get)\n",
    "\n",
    "\n",
    "# 4. Top 5 most connected east-coast rappers\n",
    "east_coast_rappers = [n for n, d in G.nodes(data=True) if d['coast'] == 'east']\n",
    "east_coast_in_degrees = {rapper: in_degrees[rapper] for rapper in east_coast_rappers}\n",
    "east_coast_out_degrees = {rapper: out_degrees[rapper] for rapper in east_coast_rappers}\n",
    "top_5_east_coast_in = sorted(east_coast_in_degrees, key=east_coast_in_degrees.get, reverse=True)[:5]\n",
    "top_5_east_coast_out = sorted(east_coast_out_degrees, key=east_coast_out_degrees.get, reverse=True)[:5]\n",
    "\n",
    "# 5. Top 5 most connected west-coast rappers\n",
    "west_coast_rappers = [n for n, d in G.nodes(data=True) if d['coast'] == 'west']\n",
    "west_coast_in_degrees = {rapper: in_degrees[rapper] for rapper in west_coast_rappers}\n",
    "west_coast_out_degrees = {rapper: out_degrees[rapper] for rapper in west_coast_rappers}\n",
    "top_5_west_coast_in = sorted(west_coast_in_degrees, key=west_coast_in_degrees.get, reverse=True)[:5]\n",
    "top_5_west_coast_out = sorted(west_coast_out_degrees, key=west_coast_out_degrees.get, reverse=True)[:5]\n",
    "\n",
    "# 6. Plot the in- and out-degree distributions\n",
    "in_degree_values = sorted([d for n, d in G.in_degree()], reverse=True)\n",
    "out_degree_values = sorted([d for n, d in G.out_degree()], reverse=True)\n",
    "\n",
    "# 7. Find the exponent for the in- and out-degree distributions\n",
    "fit_in = powerlaw.Fit(np.array(in_degree_values) + 1)\n",
    "fit_out = powerlaw.Fit(np.array(out_degree_values) + 1)\n",
    "exponent_in = fit_in.power_law.alpha\n",
    "exponent_out = fit_out.power_law.alpha\n",
    "\n",
    "# 8. Degree distribution of a random network\n",
    "p = num_links / (num_nodes * (num_nodes - 1))\n",
    "random_G = nx.erdos_renyi_graph(num_nodes, p)\n",
    "random_degrees = sorted([d for n, d in random_G.degree()], reverse=True)\n",
    "\n",
    "# Display the results\n",
    "results = pd.DataFrame.from_dict({\n",
    "    \"Metric\": [\"Number of Nodes\", \"Number of Links\", \"Top Rapper (In-Degree)\", \"Top Rapper (Out-Degree)\",\n",
    "               \"Top 5 East Coast Rappers (In-Degree)\", \"Top 5 East Coast Rappers (Out-Degree)\",\n",
    "               \"Top 5 West Coast Rappers (In-Degree)\", \"Top 5 West Coast Rappers (Out-Degree)\",\n",
    "               \"In-Degree Exponent\", \"Out-Degree Exponent\"],\n",
    "    \"Value\": [num_nodes, num_links, top_rapper_in, top_rapper_out,\n",
    "              top_5_east_coast_in, top_5_east_coast_out,\n",
    "              top_5_west_coast_in, top_5_west_coast_out,\n",
    "              exponent_in, exponent_out]\n",
    "})\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the two degree distributions two the degree distribution of a *random network* (undirected) with the same number of nodes and probability of connection *p*. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the number of nodes in the network?\n",
    "   * The number of nodes in the network is **728**.\n",
    "\n",
    "\n",
    "* What is the number of links?\n",
    "   * The number of links in the network is **5828**.\n",
    "\n",
    "\n",
    "* Who is the top connected rapper? (Report results for the in-degrees and out-degrees). Comment on your findings. Is this what you would have expected?\n",
    "   * The top rapper based on in-degrees is **Snoop Dogg**.\n",
    "   * The top rapper based on out-degrees is **Drag-On**.\n",
    "   * Comment: Snoop Dogg being the top-connected rapper based on in-degrees is unsurprising given his big influence in the hip-hop industry. An explenation could be, that snoop dog is a role model for a lot of other rappers and therefore they are referencing to him . Drag-On being the top for out-degrees, indicating that he has the most links going out from his Wikipediapage. This could mean, that his Page is very detailed ord that he collaborated with a diverse range of artists and therefore names a lot of different artists.\n",
    "\n",
    "\n",
    "* Who are the top 5 most connected east-coast rappers (again in terms of in/out-degree)?\n",
    "   * In-Degree: **Jay-Z, Nas, 50 Cent, Busta Rhymes, The Notorious B.I.G.**\n",
    "   * Out-Degree: **Drag-On, Black Thought, MC Lyte, Jahlil Beats, DJ Premier**.\n",
    "\n",
    "\n",
    "* Who are the top 5 most connected west-coast rappers (again in terms of in/out-degree)?\n",
    "   * In-Degree: **Snoop Dogg, Dr. Dre, Kendrick Lamar, Tupac Shakur, E-40**.\n",
    "   * Out-Degree: **DJ Dahi, The Game (rapper), Kendrick Lamar, Dr. Dre, Kurupt**.\n",
    "\n",
    "\n",
    "* Find the exponent (by using the `powerlaw` package) for the in- and out-degree distributions. What does it say about our network?\n",
    "   * In-Degree Exponent: **2.814131**.\n",
    "   * Out-Degree Exponent: **12.091138**.\n",
    "   * Comment: The exponents indicate the steepness of the decline in the number of nodes with a particular degree as the degree increases. Smaller exponent values suggest that there are more nodes with higher degrees. The observed values, especially for out-degrees, suggest a network that has a very skewed distribution, with few nodes having very high out-degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the in- and out-degree distributions for the whole network. \n",
    "  * Use axes that make sense for visualizing this particular distribution.\n",
    "  * What do you observe? \n",
    "  * Give a pedagogical explaination of why the in-degree distribution is different from the out-degree distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the In-Degree and Out-Degree Distributions\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.loglog(in_degree_values, 'b-', marker='o', label='In-Degree')\n",
    "plt.loglog(out_degree_values, 'r-', marker='o', label='Out-Degree')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Degree')\n",
    "plt.title('In-Degree and Out-Degree Distributions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Description of the plot***\n",
    "\n",
    "* Plot the in- and out-degree distributions for the whole network.\n",
    "   - The plots provided showcase the in-degree and out-degree distributions.\n",
    "   \n",
    "   - What do you observe?\n",
    "     - The in-degree and out-degree distributions follow a power-law distribution, which is common for many real-world networks. There is a small number of nodes (rappers) with high degrees, while most nodes have relatively low degrees.\n",
    "\n",
    "   - Give a pedagogical explanation of why the in-degree distribution is different from the out-degree distribution?\n",
    "     - **??? Maybe something with the friendship paradox ???**\n",
    "     - **or because some wikipages are longer than other ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the Degree Distribution of a Random Network\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.loglog(random_degrees, 'g-', marker='o', label='Random Network')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Degree')\n",
    "plt.title('Degree Distribution of a Random Network')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Description of the plot***\n",
    "\n",
    "* Compare the two degree distributions to the degree distribution of a *random network* (undirected) with the same number of nodes and probability of connection *p*. Comment your results.\n",
    "   - The provided random network degree distribution plot can be compared to the in-degree and out-degree distributions. \n",
    "   - Comment: The random network shows a more even distribution, suggesting that most nodes have a similar number of connections. This is in stark contrast to the observed power-law distributions for the in- and out-degrees in the rap network. The rap network has a few highly connected nodes, whereas in a random network, connections are more uniformly distributed. This showcases the uniqueness of real-world networks compared to theoretical random networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 1b_: Visualization (see lecture 5 for more hints)\n",
    "\n",
    "> * Create a nice visualization of the total (directed) network:\n",
    ">   * Color nodes according to the role;\n",
    ">   * Scale node-size according to degree;\n",
    ">   * Get node positions based on either the Force Atlas 2 algorithm, or the built-in algorithms for networkX;\n",
    ">   * Whatever else you feel like that would make the visualization nicer.\n",
    "> * Describe the structure you observe. What useful information can you decipher from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fa2 import ForceAtlas2\n",
    "\n",
    "#node color\n",
    "node_colors = []\n",
    "\n",
    "\n",
    "for key, value in nx.get_node_attributes(G, 'coast').items():\n",
    "    if value == 'east':\n",
    "        node_colors.append('blue')\n",
    "    elif value == 'west':\n",
    "        node_colors.append('red')\n",
    "    else:\n",
    "        node_colors.append('gray')\n",
    "\n",
    "\n",
    "# node size \n",
    "node_sizes = [G.degree(node) * 1.5 for node in G.nodes()]\n",
    "\n",
    "#color of the edge is based on the coast of the source node\n",
    "edge_color_list = []\n",
    "for edge in G.edges():\n",
    "    if G.nodes[edge[0]][\"coast\"] == \"west\" and G.nodes[edge[1]][\"coast\"] == \"west\":\n",
    "        edge_color_list.append(\"red\")\n",
    "    elif G.nodes[edge[0]][\"coast\"] == \"east\" and G.nodes[edge[1]][\"coast\"] == \"east\":\n",
    "        edge_color_list.append(\"blue\")\n",
    "    else:\n",
    "        edge_color_list.append(\"black\")\n",
    "\n",
    "#position\n",
    "# create a ForceAtlas2 object\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=0.5,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=10.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=0.15,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=2000)\n",
    "\n",
    "nx.draw_networkx_nodes(G, positions, node_size=node_sizes, node_color=node_colors, alpha=0.4)\n",
    "nx.draw_networkx_edges(G, positions, alpha=0.05, edge_color = edge_color_list)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe the plot and adjust it maybe ???**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word-clouds\n",
    "\n",
    "Create your own version of the word-clouds (from lecture 7). For this exercise we assume you know how to download and clean text from rappers' Wikipedia pages.\n",
    "\n",
    "Here's what you need to do:\n",
    "> * Create a word-cloud for each coast according to the novel TF-TR method. Feel free to make it as fancy as you like. Explain your process and comment on your results.\n",
    "> * For each coast, what are the 5 words with the highest TR scores? Comment on your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the code from the supporting notebook here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapper_tokens_west = pd.read_csv('rapper_tokens_west.csv', header=None).iloc[0].tolist()\n",
    "rapper_tokens_east = pd.read_csv('rapper_tokens_east.csv', header=None).iloc[0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cleaning of the corpus**\n",
    "- Remove all punctuation from your list of tokens\n",
    "\n",
    "- Set everything to lower case\n",
    "\n",
    "- Lemmatize your words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each token in the text \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist_west = FreqDist(rapper_tokens_west)\n",
    "fdist_east = FreqDist(rapper_tokens_east)\n",
    "\n",
    "# make dict out of frequency distribution\n",
    "freq_west = dict(fdist_west)\n",
    "freq_east = dict(fdist_east)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a score for each word in east. The formular is (Tf_w)/(Tf_e + c)\n",
    "# Tf_w = term frequency in west\n",
    "# Tf_e = term frequency in east\n",
    "\n",
    "# create dict with score for each word\n",
    "score_dict_east = {}\n",
    "c = 5\n",
    "for word in freq_east:\n",
    "    if word in freq_west:\n",
    "        score_dict_east[word] = freq_east[word]/(freq_west[word]+c)\n",
    "    else:\n",
    "        score_dict_east[word] = freq_east[word]/c\n",
    "\n",
    "\n",
    "# create dict with score for each word\n",
    "score_dict_west = {}\n",
    "c = 5\n",
    "for word in freq_west:\n",
    "    if word in freq_east:\n",
    "        score_dict_west[word] = freq_west[word]/(freq_east[word]+c)\n",
    "    else:\n",
    "        score_dict_west[word] = freq_west[word]/c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# create wordcloud\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.generate_from_frequencies(score_dict_east)\n",
    "wordcloud.to_image()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create wordcloud\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.generate_from_frequencies(score_dict_west)\n",
    "wordcloud.to_image()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dict by value\n",
    "import operator\n",
    "\n",
    "sorted_score_dict_east = sorted(score_dict_east.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_score_dict_west = sorted(score_dict_west.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "#print top 10 words\n",
    "print(sorted_score_dict_east[:5])\n",
    "print(sorted_score_dict_west[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the finding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Communities\n",
    "\n",
    "Find communities and their modularity (from lecture 7).\n",
    "\n",
    "Here's what you need to do:\n",
    "> * In your own words, explain what the measure \"modularity\" is, and the intuition behind the formula you use to compute it. \n",
    "> * Find communities in the network, and explain how you chose to identify the communities: Which algorithm did you use and how does it work?\n",
    "> * Comment on your results:\n",
    ">   * How many communities did you find in total?\n",
    ">   * Compute the value of modularity with the partition created by the algorithm.\n",
    ">   * Plot and/or print the distribution of community sizes (whichever makes most sense). Comment on your result.\n",
    "> * Now, partition your rappers into two communities based on which coast they represent.\n",
    ">   * What is the modularity of this partition? Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sentiment of communities\n",
    "\n",
    "Analyze the sentiment of communities (lecture 8). More tips & tricks can be found, if you take a look at Lecture 8's exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labmt=pd.read_csv('Data_Set_S1.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(labmt,tokens):\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    freq_dist = FreqDist(tokens)\n",
    "    \n",
    "    # Calculate sentiment scores for unique words\n",
    "    sentiment_scores = {}\n",
    "\n",
    "    for word, frequency in freq_dist.items():\n",
    "        matching_word = labmt[labmt['Word'] == word]\n",
    "        if not matching_word.empty:\n",
    "            sentiment = matching_word['Happiness_Average'].values[0]\n",
    "            sentiment_scores[word] = sentiment\n",
    "\n",
    "    # Calculate weighted mean sentiment\n",
    "    total_sentiment = sum(sentiment_scores[word] * freq for word, freq in freq_dist.items())\n",
    "    total_frequency = sum(freq_dist.values())\n",
    "    weighted_mean_sentiment = total_sentiment / total_frequency\n",
    "    \n",
    "    # Calculate the average sentiment score\n",
    "    if word_count > 0:\n",
    "        average_sentiment = sentiment_sum / word_count\n",
    "    else:\n",
    "        average_sentiment = 0  # If no LabMT words found, return a neutral sentiment\n",
    "    \n",
    "    return average_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of additional instructions you will need below:\n",
    "* Average the average sentiment of the nodes in each community to find a community-level sentiment.\n",
    "\n",
    "Here's what you need to do (use the LabMT wordlist approach):\n",
    "> * Calculate and store sentiment for every rapper\n",
    "> * Create a histogram of all rappers' associated sentiments.\n",
    "> * What are the 10 rappers with happiest and saddest pages?\n",
    "\n",
    "Now, compute the sentiment of each coast: \n",
    "> * Which is the happiest and which is saddest coast according to the LabMT wordlist approach? (Take the coast's sentiment to be the average sentiment of the coast's rappers' pages (disregarding any rappers with sentiment 0).\n",
    "> * Use the \"label shuffling test\" (Week 5 and 8) to test if the coast with the highest wikipedia page sentiment has a page sentiment that is significantly higher (5% confidence bound) than a randomly selected group of rappers of the same size.\n",
    "> * Does the result make sense to you? Elaborate.\n",
    "\n",
    "**Congratulations for making it to the end of the Assignment. Good luck with your independent project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Python Regular Expressions. Accessed: Oct. 25, 2023. [Online]. Available: https://developers.google.com/edu/python/regular-expressions\n",
    "\n",
    "                \n",
    "                \n",
    "[ex] Network Science by Albert-László Barabási - Chapter 3: equation (3.3). Accessed: Sep. 25, 2023. [Online]. Available: http://networksciencebook.com/  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
